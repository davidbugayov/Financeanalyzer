#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ –≤ Android –ø—Ä–æ–µ–∫—Ç–µ.
–ù–∞—Ö–æ–¥–∏—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö strings.xml –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏—Ö –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ.
"""

import os
import xml.etree.ElementTree as ET
import argparse
from collections import defaultdict
from pathlib import Path
import re
from typing import Dict, List, Tuple, Set

class StringDeduplicator:
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.string_files = []
        self.duplicates = defaultdict(list)
        self.common_strings = set()
        
    def find_string_files(self) -> List[Path]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ñ–∞–π–ª—ã strings.xml –≤ –ø—Ä–æ–µ–∫—Ç–µ, –∏—Å–∫–ª—é—á–∞—è —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏."""
        pattern = "**/res/values/strings.xml"
        all_files = list(self.project_root.glob(pattern))
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ñ–∞–π–ª—ã –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π
        string_files = [f for f in all_files if 'backup' not in f.parts]
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(string_files)} —Ñ–∞–π–ª–æ–≤ strings.xml:")
        for file in string_files:
            print(f"  - {file.relative_to(self.project_root)}")
        return string_files
    
    def parse_strings_file(self, file_path: Path) -> Dict[str, str]:
        """–ü–∞—Ä—Å–∏—Ç —Ñ–∞–π–ª strings.xml –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {name: value}."""
        strings = {}
        try:
            tree = ET.parse(file_path)
            root = tree.getroot()
            
            for string_elem in root.findall('string'):
                name = string_elem.get('name')
                value = string_elem.text or ""
                if name:
                    strings[name] = value
                    
        except ET.ParseError as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {file_path}: {e}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path}: {e}")
            
        return strings
    
    def find_duplicates(self) -> Dict[str, List[Tuple[Path, str]]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é."""
        string_files = self.find_string_files()
        value_to_files = defaultdict(list)
        
        for file_path in string_files:
            strings = self.parse_strings_file(file_path)
            for name, value in strings.items():
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ (—É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã)
                normalized_value = value.strip()
                if normalized_value:
                    value_to_files[normalized_value].append((file_path, name))
        
        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –¥—É–±–ª–∏–∫–∞—Ç—ã (–±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞)
        duplicates = {value: files for value, files in value_to_files.items() 
                     if len(files) > 1}
        
        return duplicates
    
    def find_common_strings(self) -> Set[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤–æ –≤—Å–µ—Ö –º–æ–¥—É–ª—è—Ö."""
        string_files = self.find_string_files()
        if not string_files:
            return set()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Ñ–∞–π–ª–∞
        first_file = string_files[0]
        first_strings = set(self.parse_strings_file(first_file).values())
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–æ –≤—Å–µ–º–∏ –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏
        common_strings = first_strings.copy()
        for file_path in string_files[1:]:
            file_strings = set(self.parse_strings_file(file_path).values())
            common_strings &= file_strings
        
        return common_strings
    
    def analyze_duplicates(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤—ã–≤–æ–¥–∏—Ç –æ—Ç—á–µ—Ç."""
        print("\n" + "="*60)
        print("–ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–ö–ê–¢–û–í –°–¢–†–û–ö")
        print("="*60)
        
        duplicates = self.find_duplicates()
        
        if not duplicates:
            print("–î—É–±–ª–∏–∫–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã! üéâ")
            return
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(duplicates)} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∑–Ω–∞—á–µ–Ω–∏–π:")
        print()
        
        for i, (value, files) in enumerate(duplicates.items(), 1):
            print(f"{i}. –ó–Ω–∞—á–µ–Ω–∏–µ: '{value[:50]}{'...' if len(value) > 50 else ''}'")
            print(f"   –ù–∞–π–¥–µ–Ω–æ –≤ {len(files)} —Ñ–∞–π–ª–∞—Ö:")
            for file_path, name in files:
                rel_path = file_path.relative_to(self.project_root)
                print(f"     - {rel_path} (name='{name}')")
            print()
    
    def suggest_consolidation(self):
        """–ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –ø–ª–∞–Ω –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤."""
        print("\n" + "="*60)
        print("–ü–õ–ê–ù –ö–û–ù–°–û–õ–ò–î–ê–¶–ò–ò")
        print("="*60)
        
        duplicates = self.find_duplicates()
        if not duplicates:
            return
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–æ–¥—É–ª—è–º
        module_groups = defaultdict(list)
        for value, files in duplicates.items():
            modules = set()
            for file_path, _ in files:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –º–æ–¥—É–ª—è –∏–∑ –ø—É—Ç–∏
                parts = file_path.parts
                if 'src' in parts:
                    src_index = parts.index('src')
                    if src_index > 0:
                        module_name = parts[src_index - 1]
                        modules.add(module_name)
            
            if modules:
                module_groups[tuple(sorted(modules))].append((value, files))
        
        print("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏:")
        print()
        
        for modules, items in module_groups.items():
            print(f"–ú–æ–¥—É–ª–∏: {', '.join(modules)}")
            print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(items)}")
            print("–î–µ–π—Å—Ç–≤–∏—è:")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏
            if 'ui' in modules:
                main_module = 'ui'
            elif 'common-ui' in modules:
                main_module = 'common-ui'
            else:
                main_module = modules[0]
            
            print(f"  1. –ü–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤—Å–µ –æ–±—â–∏–µ —Å—Ç—Ä–æ–∫–∏ –≤ –º–æ–¥—É–ª—å '{main_module}'")
            print(f"  2. –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª–µ–π")
            print(f"  3. –û–±–Ω–æ–≤–∏—Ç—å –∏–º–ø–æ—Ä—Ç—ã –≤ –∫–æ–¥–µ")
            print()
    
    def find_common_strings_report(self):
        """–í—ã–≤–æ–¥–∏—Ç –æ—Ç—á–µ—Ç –æ –æ–±—â–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö."""
        print("\n" + "="*60)
        print("–û–ë–©–ò–ï –°–¢–†–û–ö–ò –í–û –í–°–ï–• –ú–û–î–£–õ–Ø–•")
        print("="*60)
        
        common_strings = self.find_common_strings()
        
        if not common_strings:
            print("–û–±—â–∏—Ö —Å—Ç—Ä–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
            return
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(common_strings)} –æ–±—â–∏—Ö —Å—Ç—Ä–æ–∫:")
        print()
        
        for i, value in enumerate(sorted(common_strings), 1):
            print(f"{i}. '{value[:50]}{'...' if len(value) > 50 else ''}'")
        
        print()
        print("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –≠—Ç–∏ —Å—Ç—Ä–æ–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –º–æ–¥—É–ª–µ 'ui' –∏–ª–∏ 'common-ui'")
    
    def generate_consolidation_script(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏."""
        print("\n" + "="*60)
        print("–ì–ï–ù–ï–†–ê–¶–ò–Ø –°–ö–†–ò–ü–¢–ê –ö–û–ù–°–û–õ–ò–î–ê–¶–ò–ò")
        print("="*60)
        
        duplicates = self.find_duplicates()
        if not duplicates:
            print("–ù–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏.")
            return
        
        script_content = """#!/bin/bash
# –°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫
# –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

echo "–ù–∞—á–∏–Ω–∞–µ–º –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—é –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫..."

# –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
echo "–°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é..."
cp -r app/src/main/res/values app/src/main/res/values.backup.$(date +%Y%m%d_%H%M%S)

"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–∞
        for value, files in duplicates.items():
            if len(files) > 1:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Ñ–∞–π–ª –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π
                main_file, main_name = files[0]
                script_content += f"# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç: {value[:30]}...\n"
                
                for file_path, name in files[1:]:
                    rel_path = file_path.relative_to(self.project_root)
                    script_content += f"echo '–£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç –∏–∑ {rel_path}'\n"
                    script_content += f"# TODO: –£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É '{name}' –∏–∑ {rel_path}\n"
                
                script_content += "\n"
        
        script_content += """echo "–ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!"
echo "–ù–µ –∑–∞–±—É–¥—å—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –æ–±–Ω–æ–≤–∏—Ç—å –∏–º–ø–æ—Ä—Ç—ã –≤ –∫–æ–¥–µ."
"""
        
        script_path = self.project_root / "scripts" / "consolidate_strings.sh"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        os.chmod(script_path, 0o755)
        print(f"–°–∫—Ä–∏–ø—Ç –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {script_path}")
    
    def run_full_analysis(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑."""
        print("–ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–ö–ê–¢–û–í –°–¢–†–û–ö–û–í–´–• –†–ï–°–£–†–°–û–í")
        print("="*60)
        
        self.analyze_duplicates()
        self.find_common_strings_report()
        self.suggest_consolidation()
        self.generate_consolidation_script()

def main():
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ Android')
    parser.add_argument('--project-root', default='.', 
                       help='–ö–æ—Ä–Ω–µ–≤–∞—è –ø–∞–ø–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: —Ç–µ–∫—É—â–∞—è –ø–∞–ø–∫–∞)')
    parser.add_argument('--action', choices=['analyze', 'consolidate', 'common'], 
                       default='analyze',
                       help='–î–µ–π—Å—Ç–≤–∏–µ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è')
    
    args = parser.parse_args()
    
    deduplicator = StringDeduplicator(args.project_root)
    
    if args.action == 'analyze':
        deduplicator.run_full_analysis()
    elif args.action == 'common':
        deduplicator.find_common_strings_report()
    elif args.action == 'consolidate':
        deduplicator.generate_consolidation_script()

if __name__ == '__main__':
    main() 